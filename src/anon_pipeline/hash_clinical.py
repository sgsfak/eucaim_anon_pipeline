import shutil
from hashlib import md5
from pathlib import Path

import clevercsv
from loguru import logger


def hash_patient_id(patient_id: str, site_id: str) -> str:
    """
    Anonymizes the given patient ID according to the "CTP" algorithm.

    The hashed string is generated by concatenating the site ID and patient ID, hashing the
    result using the MD5 algorithm, and then truncating the result to the given maximum
    length. The hashed string is then prefixed with the given prefix.

    (see https://github.com/johnperry/CTP/blob/ea8639754cf38c50cdc9999170e192bd101fd5d7/source/java/org/rsna/ctp/stdstages/anonymizer/AnonymizerFunctions.java#L186)
    """
    if not patient_id:
        patient_id = "null"
    else:
        patient_id = patient_id.strip()

    # This method is used by the CTP anonymizer to hash patient IDs, and is included here to provide
    # compatibility with that system. Basically it computes the MD5 hash and then interprets the result
    # as an (big) integer in the "big endian" format.
    # (see https://github.com/johnperry/Util/blob/cf5a160b31abdc67f4767a932d70661257803320/source/java/org/rsna/util/DigestUtil.java#L91)
    s = f"[{site_id}]{patient_id}"
    h = md5(s.encode()).digest()
    return str(int.from_bytes(h, byteorder="big"))


def _parse_and_hash_clinical_csv(
    input_file: Path,
    output_file: Path,
    site_id: str,
    prefix: str = "EUCAIM-",
) -> None:
    """
    Parses a clinical CSV file and hashes the patient IDs using the CTP algorithm.

    Args:
        input_file: The path to the input CSV file.
        output_file: The path to the output CSV file.
        site_id: The 'site ID' (actually "pepper") to use for hashing.
        prefix: The prefix to use for the hashed patient IDs, defaults to "EUCAIM-"
                Note that this should be aligned with the anon.script !!!

    Returns:
        None
    """
    logger.info(f"Parsing and hashing clinical CSV file {input_file}")
    with (
        open(input_file, "r", newline="") as fp,
        open(output_file, "w", newline="") as fp_out,
    ):
        dialect = clevercsv.Sniffer().sniff(fp.read(10000))
        if dialect is None:
            logger.error(f"Failed to detect CSV dialect for file {input_file}")
        logger.info(f"Detected CSV dialect: {dialect.to_dict()}")
        fp.seek(0)
        reader = clevercsv.reader(fp, dialect)
        rows = list(reader)

        # What will be the CSV dialect used for the output file? We have two sane options:
        # RFC4180, which is kind of standard, or the dialect used in the input file
        # I opted for RFC4180 to ensure consistency.
        # Note: For some reason in CleverCSV the 'excel dialect corresponds to RFC4180!
        # See https://clevercsv.readthedocs.io/en/latest/source/clevercsv.html#clevercsv.wrappers.write_table
        writer = clevercsv.writer(fp_out, "excel")

        # We assume that the first row in the input file contains the header, so
        # we write it to the output file as is:
        writer.writerow(rows[0])

        for row in rows[1:]:
            hashed_id = hash_patient_id(row[0], site_id)
            new_patient_id = f"{prefix}{hashed_id}"
            writer.writerow([new_patient_id, *row[1:]])
        logger.info(f"Wrote {len(rows)} rows to {output_file} in RFC4180 CSV format")


def hash_clinical_csvs(
    input_dir: Path,
    output_dir: Path,
    *,
    site_id: str,
    ignore_prefix: str = "_",
) -> None:
    """
    Checks the input_dir and finds the clinical CSV files in it. Then, for each file,
    it parses and hashes the patient IDs. It only checks files with the ".csv" extension
    located directly in the input directory (it does not search in subdirectories), and
    skips files that start with the given `ignore_prefix`. Any csv file with a name that
    starts with the given `ignore_prefix` is just copied to the output directory.
    """
    csvs = list(c for c in input_dir.glob("*.csv") if c.is_file())
    if not csvs:
        logger.warning("No CSV found in input directory")
        return

    csvs_to_copied = []
    csvs_to_be_hashed = []
    for csv in csvs:
        if csv.name.startswith(ignore_prefix):
            csvs_to_copied.append(csv)
        else:
            csvs_to_be_hashed.append(csv)
    logger.info(
        f"Found {len(csvs_to_be_hashed)} CSV file(s) in {input_dir} to be hashed "
        f"and {len(csvs_to_copied)} CSV file(s) to be copied"
    )
    for input_clinical_csv in csvs_to_be_hashed:
        output_clinical_csv = output_dir / input_clinical_csv.name
        _parse_and_hash_clinical_csv(input_clinical_csv, output_clinical_csv, site_id)
    for csv in csvs_to_copied:
        output_csv = output_dir / csv.name
        shutil.copy(csv, output_csv)
