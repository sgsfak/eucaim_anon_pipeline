import shutil
from functools import cache
from hashlib import md5
from pathlib import Path
from typing import Callable, Iterable, Literal
from xml.etree import ElementTree as ET

import clevercsv
from loguru import logger

from .defaults import (
    DEFAULT_IGNORE_CSV_PREFIX,
    DEFAULT_PATIENT_ID_PREFIX,
    DEFAULT_STUDIES_METADATA_CSV,
    DEFAULT_UIDROOT,
)


@cache
def _anonymization_info(anon_script: str):
    """Returns the params and the directives for anonymization for StudyInstanceUID, SeriesInstanceUID, and PatientID"""
    tree = ET.parse(anon_script)
    root = tree.getroot()
    params = [(e.attrib["t"], e.text) for e in tree.getroot().findall("p")]
    es = [
        (e.attrib["n"], e.text)
        for e in tree.getroot().findall("e")
        if e.attrib["n"] in ("StudyInstanceUID", "SeriesInstanceUID", "PatientID")
    ]
    return params, es


def anonymization_params(anon_script: str) -> dict[str, str | None]:
    params, _ = _anonymization_info(anon_script)
    return params


def anonymization_directives(anon_script: str) -> dict[str, str | None]:
    _, es = _anonymization_info(anon_script)
    return es


def _hashUID(prefix: str, uid: str):
    """
    https://github.com/johnperry/CTP/blob/master/source/java/org/rsna/ctp/stdstages/anonymizer/AnonymizerFunctions.java#L442

    """
    prefix = prefix.strip()
    if prefix != "" and not prefix.endswith("."):
        prefix += "."
    h = md5(uid.encode()).digest()
    hashString = str(int.from_bytes(h, byteorder="big"))
    extra = "9" if hashString.startswith("0") else ""
    newuid = prefix + extra + hashString
    if len(newuid) > 64:
        newuid = newuid[0:64]
    return newuid


def hash_uid_using_anon_patient_id(
    *,
    uid: str,
    prefix: str,
    anonymized_patient_id: str,
) -> str:
    """This function implements the '@hashuid(@UIDROOT,this,PatientID)' directive of CTP
    The supplied `prefix` is the UID root to be used as a prefix for the generated UID. `uid` is the value of "this"
    tag (e.g. the non-anonymized value of StudyInstanceUID) while `anonymized_patient_id` is the anonymized value
    of the PatientID tag.

    See https://github.com/johnperry/CTP/blob/361e90f8032fbb343000f3397159ae24a001229a/source/java/org/rsna/ctp/stdstages/anonymizer/dicom/DICOMAnonymizer.java#L1359
    """

    if not prefix.endswith("."):
        prefix += "."
    uid += anonymized_patient_id

    return _hashUID(prefix, uid)


def hash_patient_id(patient_id: str, site_id: str, prefix: str) -> str:
    """
    Anonymizes the given patient ID according to the "CTP" algorithm.

    The hashed string is generated by concatenating the site ID and patient ID, hashing the
    result using the MD5 algorithm, and then interpreting the result as an (big) integer in
    the "big endian" format. The hashed string is then prefixed with the given prefix.

    (see https://github.com/johnperry/CTP/blob/ea8639754cf38c50cdc9999170e192bd101fd5d7/source/java/org/rsna/ctp/stdstages/anonymizer/AnonymizerFunctions.java#L186)
    """
    if not patient_id:
        patient_id = "null"
    else:
        patient_id = patient_id.strip()

    # This method is used by the CTP anonymizer to hash patient IDs, and is included here to provide
    # compatibility with that system. Basically it computes the MD5 hash and then interprets the result
    # as an (big) integer in the "big endian" format.
    # (see https://github.com/johnperry/Util/blob/cf5a160b31abdc67f4767a932d70661257803320/source/java/org/rsna/util/DigestUtil.java#L91)
    s = f"[{site_id}]{patient_id}"
    h = md5(s.encode()).digest()
    hashed_pid = str(int.from_bytes(h, byteorder="big"))
    return f"{prefix}{hashed_pid}"


@cache
def _clinical_hasher_factory(
    *,
    site_id: str,
    prefix: str = DEFAULT_PATIENT_ID_PREFIX,
) -> Callable[[list[str]], list[str]]:
    """
    Returns a "mapper" that given a list of strings (a row), hashes the patient IDs (assumed to be in the first column)
    using the CTP algorithm and returns the transformed row.

    Args:
        site_id: The 'site ID' (actually "pepper") to use for hashing.
        prefix: The prefix to use for the hashed patient IDs, defaults to "EUCAIM-"
                Note that this should be aligned with the anon.script !!!
    Returns:
        the mapper function
    """

    def mapper(row: list[str]) -> list[str]:
        new_patient_id = hash_patient_id(row[0], site_id, prefix)
        return [new_patient_id, *row[1:]]

    return mapper


@cache
def _studies_hasher_factory(
    *,
    prefix: str,
    site_id: str,
    uidroot: str,
) -> Callable[[list[str]], list[str]]:
    """
    Returns a "mapper" that given a list of strings (a row), hashes the patient IDs (assumed to be in the first column)
    using the CTP algorithm and returns the transformed row.

    Args:
        site_id: The 'site ID' (actually "pepper") to use for hashing.
        prefix: The prefix to use for the hashed patient IDs, defaults to "EUCAIM-"
                Note that this should be aligned with the anon.script !!!
    Returns:
        the mapper function
    """

    def mapper(row: list[str]) -> list[str]:
        new_patient_id = hash_patient_id(row[0], site_id, prefix)

        hashed_study_uid = hash_uid_using_anon_patient_id(
            uid=row[1],
            prefix=uidroot,
            anonymized_patient_id=new_patient_id,
        )

        return [new_patient_id, hashed_study_uid, *row[2:]]

    return mapper


def _parse_and_hash_csv(
    input_file: Path,
    output_file: Path,
    mapper: Callable[[list[str]], list[str]],
) -> None:
    """
    Parses an input CSV file, applies the given `mapper` function to each row, and writes the result to the output file.

    Args:
        input_file: The path to the input CSV file.
        output_file: The path to the output CSV file.
    Returns:
        None
    """
    logger.info(f"Parsing and hashing CSV file {input_file}")
    with (
        open(input_file, "r", newline="") as fp,
        open(output_file, "w", newline="") as fp_out,
    ):
        dialect = clevercsv.Sniffer().sniff(fp.read(10000))
        if dialect is None:
            logger.error(f"Failed to detect CSV dialect for file {input_file}")
        logger.info(f"Detected CSV dialect: {dialect.to_dict()}")
        fp.seek(0)
        reader = clevercsv.reader(fp, dialect)
        rows: list[list[str]] = list(reader)

        # What will be the CSV dialect used for the output file? We have two sane options:
        # RFC4180, which is kind of standard, or the dialect used in the input file
        # I opted for RFC4180 to ensure consistency.
        # Note: For some reason in CleverCSV the 'excel dialect corresponds to RFC4180!
        # See https://clevercsv.readthedocs.io/en/latest/source/clevercsv.html#clevercsv.wrappers.write_table
        writer = clevercsv.writer(fp_out, "excel")

        # We assume that the first row in the input file contains the header, so
        # we write it to the output file as is:
        writer.writerow(rows[0])

        for row in rows[1:]:
            writer.writerow(mapper(row))
        logger.info(f"Wrote {len(rows)} rows to {output_file} in RFC4180 CSV format")


def hash_clinical_csvs(
    input_dir: Path,
    output_dir: Path,
    *,
    site_id: str,
    uidroot: str = DEFAULT_UIDROOT,
    ignore_prefix: str = DEFAULT_IGNORE_CSV_PREFIX,
) -> None:
    """
    Checks the input_dir and finds the clinical CSV files in it. Then, for each file,
    it parses and hashes the patient IDs. It only checks files with the ".csv" extension
    located directly in the input directory (it does not search in subdirectories), and
    skips files that start with the given `ignore_prefix`. Any csv file with a name that
    starts with the given `ignore_prefix` is just copied to the output directory.
    """
    csvs = list(c for c in input_dir.glob("*.csv") if c.is_file())
    if not csvs:
        logger.warning("No CSV found in input directory")
        return

    csvs_to_copied = []
    csvs_to_be_hashed = []
    for csv in csvs:
        if csv.name.startswith(ignore_prefix):
            csvs_to_copied.append(csv)
        else:
            csvs_to_be_hashed.append(csv)
    logger.info(
        f"Found {len(csvs_to_be_hashed)} CSV file(s) in {input_dir} to be hashed "
        f"and {len(csvs_to_copied)} CSV file(s) to be copied"
    )
    prefix: str = DEFAULT_PATIENT_ID_PREFIX
    for input_clinical_csv in csvs_to_be_hashed:
        output_clinical_csv = output_dir / input_clinical_csv.name
        mapper = _clinical_hasher_factory(prefix=prefix, site_id=site_id)
        if input_clinical_csv.name == DEFAULT_STUDIES_METADATA_CSV:
            mapper = _studies_hasher_factory(
                prefix=prefix, site_id=site_id, uidroot=uidroot
            )
        _parse_and_hash_csv(
            input_clinical_csv,
            output_clinical_csv,
            mapper,
        )
    for csv in csvs_to_copied:
        output_csv = output_dir / csv.name
        shutil.copy(csv, output_csv)
